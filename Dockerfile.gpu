# RefChecker Docker Image - GPU Variant
# Includes vLLM and PyTorch for local model inference with NVIDIA CUDA
#
# Build: docker build -f Dockerfile.gpu -t refchecker:gpu .
# Run:   docker run --gpus all -p 8000:8000 refchecker:gpu

# =============================================================================
# Stage 1: Build the frontend
# =============================================================================
FROM node:20-slim AS node-builder

WORKDIR /build

# Copy package files first for better layer caching
COPY web-ui/package*.json ./

# Install dependencies
RUN npm ci --no-audit --no-fund

# Copy source and build
COPY web-ui/ ./
RUN npm run build

# =============================================================================
# Stage 2: Runtime image with CUDA
# =============================================================================
FROM nvidia/cuda:12.1-runtime-ubuntu22.04 AS runtime

LABEL org.opencontainers.image.title="RefChecker GPU"
LABEL org.opencontainers.image.description="Academic paper reference validation tool with GPU support for local LLM inference"
LABEL org.opencontainers.image.source="https://github.com/markrussinovich/refchecker"
LABEL org.opencontainers.image.licenses="MIT"

# Prevent interactive prompts during package installation
ENV DEBIAN_FRONTEND=noninteractive

# Install Python 3.11 and runtime dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    # Python
    python3.11 \
    python3.11-venv \
    python3.11-dev \
    python3-pip \
    # Build tools for some Python packages
    gcc \
    g++ \
    # PDF processing
    libmupdf-dev \
    # Fonts for thumbnail generation
    fonts-dejavu-core \
    # Health check
    curl \
    # Git for some pip installs
    git \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Set Python 3.11 as default
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1 && \
    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1

# Create non-root user
RUN groupadd --gid 1000 refchecker && \
    useradd --uid 1000 --gid refchecker --shell /bin/bash --create-home refchecker

# Set working directory
WORKDIR /app

# Create and activate virtual environment
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Upgrade pip
RUN pip install --no-cache-dir --upgrade pip wheel setuptools

# Install PyTorch with CUDA support first (large download, cached early)
RUN pip install --no-cache-dir \
    torch>=2.0.0 \
    --index-url https://download.pytorch.org/whl/cu121

# Install vLLM and HuggingFace dependencies
RUN pip install --no-cache-dir \
    vllm>=0.3.0 \
    huggingface_hub>=0.17.0 \
    transformers>=4.30.0 \
    accelerate>=0.20.0

# Install remaining Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application source
COPY --chown=refchecker:refchecker src/ ./src/
COPY --chown=refchecker:refchecker backend/ ./backend/
COPY --chown=refchecker:refchecker pyproject.toml ./

# Copy bundled frontend from node builder
COPY --from=node-builder --chown=refchecker:refchecker /build/dist ./backend/static/

# Create directories for persistent storage and model cache
RUN mkdir -p /app/data /app/models && \
    chown -R refchecker:refchecker /app/data /app/models

# Environment variables
ENV PYTHONPATH="/app/src:/app" \
    PYTHONUNBUFFERED=1 \
    REFCHECKER_DATA_DIR="/app/data" \
    HF_HOME="/app/models" \
    TRANSFORMERS_CACHE="/app/models" \
    # CUDA settings
    NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Switch to non-root user
USER refchecker

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=30s --retries=3 \
    CMD curl -f http://localhost:8000/api/health || exit 1

# Default command
ENTRYPOINT ["python", "-m", "backend"]
CMD ["--host", "0.0.0.0", "--port", "8000"]
